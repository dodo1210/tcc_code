{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('training_data.csv')\n",
    "test = pd.read_csv('test_data.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/douglas/Documentos/tcc_code/analise_estatistica/resultados_sem_acordes_intervalos.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/douglas/Documentos/tcc_code/analise_estatistica/resultados_sem_acordes_intervalos.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 99 lines in 0.015607 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 99 lines in 0.015607 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,int,int,float,float,float,float,float,float,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/douglas/Documentos/tcc_code/analise_estatistica/resultados_sem_acordes_intervalos.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/douglas/Documentos/tcc_code/analise_estatistica/resultados_sem_acordes_intervalos.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 99 lines in 0.015582 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 99 lines in 0.015582 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = graphlab.SFrame('resultados_sem_acordes_intervalos.csv')\n",
    "treino,teste = data.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "treino.save('training_data.csv', format='csv')\n",
    "teste.save('test_data.csv', format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('musica', <tf.Tensor 'IteratorGetNext_2:7' shape=(?,) dtype=string>), ('tom', <tf.Tensor 'IteratorGetNext_2:9' shape=(?,) dtype=int32>), ('modo', <tf.Tensor 'IteratorGetNext_2:6' shape=(?,) dtype=int32>), ('tempo(bpm)', <tf.Tensor 'IteratorGetNext_2:8' shape=(?,) dtype=float32>), ('Intensidade_media_da_musica', <tf.Tensor 'IteratorGetNext_2:0' shape=(?,) dtype=float32>), ('espectograma_medio', <tf.Tensor 'IteratorGetNext_2:5' shape=(?,) dtype=float32>), ('espectograma_maximo', <tf.Tensor 'IteratorGetNext_2:4' shape=(?,) dtype=float32>), ('energia_medio_rmse', <tf.Tensor 'IteratorGetNext_2:3' shape=(?,) dtype=float32>), ('brilhantismo_medio_spectral_centroid', <tf.Tensor 'IteratorGetNext_2:2' shape=(?,) dtype=float32>), ('Status', <tf.Tensor 'IteratorGetNext_2:1' shape=(?,) dtype=int32>)])\n"
     ]
    }
   ],
   "source": [
    "# load a csv\n",
    "CSV_PATH_TREINO = 'training_data.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH_TREINO, batch_size=32)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "print(next) # next is a dict with key=columns names and value=column data\n",
    "inputs, labels = next, next['Status']\n",
    "with  tf.Session() as sess:\n",
    "    sess.run([inputs, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('musica', <tf.Tensor 'IteratorGetNext_5:7' shape=(?,) dtype=string>), ('tom', <tf.Tensor 'IteratorGetNext_5:9' shape=(?,) dtype=int32>), ('modo', <tf.Tensor 'IteratorGetNext_5:6' shape=(?,) dtype=int32>), ('tempo(bpm)', <tf.Tensor 'IteratorGetNext_5:8' shape=(?,) dtype=float32>), ('Intensidade_media_da_musica', <tf.Tensor 'IteratorGetNext_5:0' shape=(?,) dtype=float32>), ('espectograma_medio', <tf.Tensor 'IteratorGetNext_5:5' shape=(?,) dtype=float32>), ('espectograma_maximo', <tf.Tensor 'IteratorGetNext_5:4' shape=(?,) dtype=float32>), ('energia_medio_rmse', <tf.Tensor 'IteratorGetNext_5:3' shape=(?,) dtype=float32>), ('brilhantismo_medio_spectral_centroid', <tf.Tensor 'IteratorGetNext_5:2' shape=(?,) dtype=float32>), ('Status', <tf.Tensor 'IteratorGetNext_5:1' shape=(?,) dtype=int32>)])\n"
     ]
    }
   ],
   "source": [
    "# load a csv\n",
    "CSV_PATH_TESTE = 'training_data.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH_TESTE, batch_size=32)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "print(next) # next is a dict with key=columns names and value=column data\n",
    "inputs, labels = next, next['Status']\n",
    "with  tf.Session() as sess:\n",
    "    sess.run([inputs, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape and normalize training data\n",
    "train = pd.read_csv(\"training_data.csv\").values\n",
    "test  = pd.read_csv(\"test_data.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'comment_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-9f73e540c727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_comments_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msubmission_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmission_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/douglas/anaconda2/envs/oto/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/douglas/anaconda2/envs/oto/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/douglas/anaconda2/envs/oto/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/douglas/anaconda2/envs/oto/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/douglas/anaconda2/envs/oto/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'comment_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('training_data.csv')\n",
    "submission_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "train_comments_orig = train_df['comment_text']\n",
    "submission_comments = submission_df['comment_text']\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_comments, test_comments, train_true, test_true = train_test_split(train_comments_orig, train_df[labels])\n",
    "test_true_matrix = test_true.as_matrix()\n",
    "\n",
    "print(len(train_comments))\n",
    "print(len(train_true))\n",
    "print(len(test_comments))\n",
    "print(len(test_true))\n",
    "\n",
    "# HELPER CLASSES\n",
    "\n",
    "class CommentsEmbedder():\n",
    "    \n",
    "    def __init__(self, fit_comments):\n",
    "        self.fit_comments = fit_comments\n",
    "        self.num_words = 10000\n",
    "\n",
    "        self.vectorizer = vectorizer = TfidfVectorizer(\n",
    "                                        analyzer='word', \n",
    "                                        sublinear_tf=True,\n",
    "                                        strip_accents='unicode',\n",
    "                                        token_pattern=r'\\w{1,}',\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range=(1, 3),\n",
    "                                        max_features=self.num_words)\n",
    "        self.tfidf = self.vectorizer.fit(self.fit_comments)\n",
    "        \n",
    "    '''transform array of comments to tfidf matrix'''\n",
    "    def transform(self, comments):\n",
    "        sparse = self.tfidf.transform(comments)\n",
    "        return sparse.todense()\n",
    "        \n",
    "class CommentData():\n",
    "    \n",
    "    def __init__(self, comments, y_true=None):\n",
    "        \n",
    "        self.comments = comments\n",
    "        self.y_true = y_true\n",
    "        self.i = 0\n",
    "        self.do_next_batch = True\n",
    "        \n",
    "        \n",
    "    \n",
    "    def next_batch(self,batch_size):\n",
    "        if self.i + batch_size >= len(self.comments):\n",
    "            new_i = len(self.comments) + 1\n",
    "            self.do_next_batch = False\n",
    "        else:\n",
    "            new_i = self.i + batch_size\n",
    "        \n",
    "        batch_x = self.comments[self.i:new_i]\n",
    "        \n",
    "        if self.y_true is not None:\n",
    "            batch_y = self.y_true[self.i:new_i].as_matrix()\n",
    "            self.i = new_i\n",
    "            return batch_x, batch_y    \n",
    "        else:\n",
    "            self.i = new_i\n",
    "            return batch_x\n",
    "        \n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv1d(x, W):\n",
    "    # x is input tensor --> [batch, num_words, in_channels]\n",
    "    # W is the kernel --> [filter width, in_channels, out_channels]\n",
    "    return tf.nn.conv1d(x,W, stride=1, padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    # x is input tensor --> [batch, num_words, in_channels]\n",
    "    return tf.nn.pool(x, window_shape=[1], pooling_type='MAX', padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[2]])\n",
    "    \n",
    "    return tf.nn.relu(conv1d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    \n",
    "    return tf.matmul(input_layer,W) + b\n",
    "    \n",
    "#### EMBED\n",
    "\n",
    "print('Fitting vectorizer...')\n",
    "        \n",
    "comment_embedder = CommentsEmbedder(train_comments_orig)\n",
    "\n",
    "print('Transforming test comments...')\n",
    "\n",
    "test_comment_matrix = comment_embedder.transform(test_comments)\n",
    "\n",
    "# GRAPH PLACEHOLDERS AND SOME HYPERPARAMETERS\n",
    "\n",
    "batchSize = 15\n",
    "numClasses = len(labels)\n",
    "iterations = 10635\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print('Max possible number of iterations: {}'.format(int(len(train_comments)/batchSize)))\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, shape=[None, comment_embedder.num_words])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, numClasses])\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# LAYERS\n",
    "\n",
    "convo_1_num_features = 10\n",
    "convo_1_filter_width = 2\n",
    "\n",
    "convo_2_num_features = 20\n",
    "convo_2_filter_width = 2\n",
    "\n",
    "convo_input = tf.reshape(input_data, [-1, comment_embedder.num_words ,1]) \n",
    "\n",
    "convo_1 = convolutional_layer(convo_input, shape=[convo_1_filter_width, 1, convo_1_num_features])\n",
    "\n",
    "convo_1_pooling = max_pool(convo_1)\n",
    "\n",
    "convo_2 = convolutional_layer(convo_1_pooling, shape=[convo_2_filter_width, convo_1_num_features, convo_2_num_features])\n",
    "\n",
    "#convo_2 = convolutional_layer(convo_input, shape=[convo_2_filter_width, 1, convo_2_num_features])\n",
    "\n",
    "convo_2_pooling = max_pool(convo_2)\n",
    "\n",
    "convo_2_flat = tf.reshape(convo_2_pooling, shape=[-1, comment_embedder.num_words*convo_2_num_features])\n",
    "\n",
    "\n",
    "# DROPOUT\n",
    "\n",
    "dropout = tf.nn.dropout(convo_2_flat, keep_prob=hold_prob)\n",
    "\n",
    "\n",
    "normal_full = normal_full_layer(dropout, numClasses)\n",
    "y_pred = tf.sigmoid(normal_full)\n",
    "\n",
    "\n",
    "# LOSS FUNCTION\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.log_loss(y_true, y_pred))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "\n",
    "\n",
    "# SUBMISSION PREDICTER\n",
    "\n",
    "predict = y_pred\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    data = CommentData(train_comments, train_true)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        if data.do_next_batch == False:\n",
    "            break\n",
    "        \n",
    "        batch_x , batch_y = data.next_batch(batchSize)\n",
    "        \n",
    "        batch_x = comment_embedder.transform(batch_x)\n",
    "        \n",
    "        sess.run(train,feed_dict={input_data:batch_x, y_true:batch_y, hold_prob:0.8})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Test Set Loss:')\n",
    "            \n",
    "            # get random sample of 5000 from test set\n",
    "            test_indexes = np.random.randint(0, len(test_comments)-1, size=5000)\n",
    "            \n",
    "            test_comment_matrix_5000 = [test_comment_matrix[i] for i in test_indexes]\n",
    "            test_comment_matrix_5000 = np.reshape(test_comment_matrix_5000, [5000, -1])\n",
    "            \n",
    "            test_true_5000 = [test_true_matrix[i] for i in test_indexes]\n",
    "            test_true_5000 = np.reshape(test_true_5000, [5000, -1])\n",
    "            \n",
    "            print(sess.run(loss,feed_dict={input_data:test_comment_matrix_5000, y_true:test_true_5000, hold_prob:1.0}))\n",
    "            print('\\n')\n",
    "     \n",
    "    print('Finished training, making predictions...')   \n",
    "    \n",
    "    # overwrite train data to save memory\n",
    "    data = None\n",
    "    submission_data = CommentData(submission_comments, None)\n",
    "    \n",
    "    # clear test matrix\n",
    "    test_comment_matrix = None\n",
    "    \n",
    "    last_submission_ix = 0\n",
    "    \n",
    "    while submission_data.do_next_batch == True:\n",
    "        \n",
    "        batch_x = submission_data.next_batch(batchSize)\n",
    "        \n",
    "        batch_x = comment_embedder.transform(batch_x)\n",
    "        \n",
    "        if submission_data.i%10000 == 0:\n",
    "            print('On prediction {}'.format(submission_data.i))\n",
    "\n",
    "        submission_pred = sess.run(predict,feed_dict={input_data:batch_x, hold_prob:1.0})\n",
    "        \n",
    "        try:\n",
    "          pred_df\n",
    "        except NameError:\n",
    "          pred_df = pd.DataFrame(data=submission_pred, index=submission_df['id'][:submission_data.i], columns=labels)\n",
    "          last_submission_ix = submission_data.i\n",
    "        else:\n",
    "          pred_df = pred_df.append(pd.DataFrame(data=submission_pred, index=submission_df['id'][last_submission_ix:submission_data.i], columns=labels))\n",
    "          last_submission_ix = submission_data.i\n",
    "\n",
    "    pred_df.to_csv('submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
